{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "live-project-02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUNREpEuurVg"
      },
      "source": [
        "## Section 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VklM30ZqJHWE"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os, glob\n",
        "\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx694ZJTJNiF"
      },
      "source": [
        "contraction_map = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "                   \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                   \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                   \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                   \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
        "                   \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
        "                   \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                   \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
        "                   \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                   \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "                   \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                   \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "                   \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                   \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                   \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
        "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                   \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "                   \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                   \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
        "                   \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "                   \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "                   \"there'd\": \"there would\",\n",
        "                   \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
        "                   \"they'd've\": \"they would have\",\n",
        "                   \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "                   \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                   \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "                   \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                   \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "                   \"what're\": \"what are\",\n",
        "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
        "                   \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
        "                   \"who've\": \"who have\",\n",
        "                   \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "                   \"won't've\": \"will not have\",\n",
        "                   \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                   \"y'all\": \"you all\",\n",
        "                   \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\",\n",
        "                   \"y'all've\": \"you all have\",\n",
        "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "                   \"you'll've\": \"you will have\",\n",
        "                   \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB41y8qzI5bf",
        "outputId": "5513dcb3-3072-4e5b-d5cf-f229b52bb1e3"
      },
      "source": [
        "files = [\n",
        "    {\n",
        "        'name':'16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json'\n",
        "     },\n",
        "     {\n",
        "        'name': '16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json'\n",
        "     }\n",
        "]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "max_len_text = 600\n",
        "max_len_target = 30\n",
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdrfeZpMI5_S"
      },
      "source": [
        "def read_records_from_files(files):\n",
        "    dataset = []\n",
        "    target = []\n",
        "    for file in files:\n",
        "        dat, tar = read_records(file)\n",
        "        for d, t in zip(dat, tar):\n",
        "            dataset.append(d)\n",
        "            target.append(t)\n",
        "\n",
        "    return dataset, target\n",
        "\n",
        "\n",
        "def read_records(file):\n",
        "    dataset = []\n",
        "    target = []\n",
        "    with open(f\"{file['name']}\") as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line)\n",
        "            dataset.append(record['text'])\n",
        "            target.append(record['title'])\n",
        "\n",
        "    return dataset, target\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = text.split()  # convert have'nt -> have not\n",
        "    for i in range(len(text)):\n",
        "        word = text[i]\n",
        "        if word in contraction_map:\n",
        "            text[i] = contraction_map[word]\n",
        "    text = \" \".join(text)\n",
        "    text = text.split()\n",
        "    newtext = []\n",
        "    for word in text:\n",
        "        if word not in stop_words:\n",
        "            newtext.append(word)\n",
        "    text = \" \".join(newtext)\n",
        "    text = text.replace(\"'s\", '')  # convert your's -> your\n",
        "    text = re.sub(r'\\(.*\\)', '', text)  # remove (words)\n",
        "    text = re.sub(r'[^a-zA-Z0-9. ]', '', text)  # remove punctuations\n",
        "    text = re.sub(r'\\.', ' . ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_temp_df(dataset, target):\n",
        "    short_text = []\n",
        "    short_summary = []\n",
        "    \n",
        "    for i in range(len(dataset)):\n",
        "        if len(target[i].split()) <= max_len_target and len(dataset[i].split()) <= max_len_text:\n",
        "            short_text.append(dataset[i])\n",
        "            short_summary.append(target[i])\n",
        "    return pd.DataFrame({'text': short_text, 'summary': short_summary})\n",
        "\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "def readData(text, summary):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[text[i], summary[i]] for i in range(len(text))]\n",
        "\n",
        "    input_lang = Lang(text)\n",
        "    output_lang = Lang(summary)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = readData(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aMN6QfqJidD"
      },
      "source": [
        "dataset, target = read_records_from_files(files)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtC8AGHhJqhJ"
      },
      "source": [
        "X = [preprocess(word) for word in dataset]\n",
        "Y = [preprocess(word) for word in target]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqflJYMxJ2Vq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "31858857-57f0-41ce-f7c3-08f76d55c18b"
      },
      "source": [
        "temp_df = get_temp_df(dataset, target)\n",
        "new_df = temp_df[temp_df['summary'].str.strip().astype(bool)]\n",
        "df = new_df[new_df['text'].str.strip().astype(bool)]\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FDA launches app for health care professionals...</td>\n",
              "      <td>FDA launches app for health care professionals...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Of all of Regina Yan ’s many traits, an open m...</td>\n",
              "      <td>C-Suite Awards: Regina Yan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The CURE ID app allows clinicians to share and...</td>\n",
              "      <td>FDA Launches Infectious Disease Crowdsourcing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The DSB is composed of representatives from tw...</td>\n",
              "      <td>Drug Safety Oversight Board</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Centre for Health Protection (CHP) of the ...</td>\n",
              "      <td>Suspected MERS case reported</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                            summary\n",
              "0  FDA launches app for health care professionals...  FDA launches app for health care professionals...\n",
              "1  Of all of Regina Yan ’s many traits, an open m...                         C-Suite Awards: Regina Yan\n",
              "2  The CURE ID app allows clinicians to share and...  FDA Launches Infectious Disease Crowdsourcing ...\n",
              "3  The DSB is composed of representatives from tw...                        Drug Safety Oversight Board\n",
              "4  The Centre for Health Protection (CHP) of the ...                       Suspected MERS case reported"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouFJc3omJ7C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84b75b3-119d-4fe7-8e6d-d992c1832550"
      },
      "source": [
        "input_lang, output_lang, pairs = prepareData(X, Y)\n",
        "pairs[:5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 159 sentence pairs\n",
            "Counting words...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['dublin swine healthcare market  growth trends forecast  5 . 2 . 2 coccidiosis 5 . 2 . 3 respiratory diseases 5 . 2 . 4 swine dysentery 5 . 2 . 5 porcine parvovirus 5 . 2 . 6 others 5 . 3 geography 5 . 3 . 1 north america 5 . 3 . 2 europe 5 . 3 . 3 asiapacific 5 . 3 . 4 middle east  africa 5 . 3 . 5 south america 6 competitive landscape 6 . 1 company profiles 6 . 1 . 1 abaxis 6 . 1 . 2 bayer animal health 6 . 1 . 3 boehringer ingelheim 6 . 1 . 4 ceva animal health inc .  6 . 1 . 5 elanco 6 . 1 . 6 idvet 6 . 1 . 7 merck animal health 6 . 1 . 8 merial 6 . 1 . 9 vetoquinol s . a .  6 . 1 . 10 virbac 6 . 1 . 11 zoetis animal healthcare 7 market opportunities future trends information report visit httpswww . researchandmarkets . comrshhuje research markets also offers custom research services providing focused comprehensive tailored research .  contact researchandmarkets . com laura wood senior press manager pressresearchandmarkets . com e . s . t office hours call 19173000470 u . s . can toll free call 18005268630 gmt office hours call 35314168900',\n",
              "  'global swine healthcare market products diseases  geography  forecast 2024'],\n",
              " ['fda launches app health care professionals report novel uses existing medicines patients difficulttotreat infectious diseases fda launches app health care professionals report novel uses existing medicines patients difficulttotreat infectious diseases authors fda pinworms red howler monkey  yue malauer yu zheng miroslav maleevi brigitte von brunn gunter fischer albrecht von brunnabstractthe wellknown immunosuppressive drug cyclosporin inhibits replication various viruses including coronaviruses binding cellular cyclophilins thus inactivating cistrans peptidylprolyl isomerase function .  viral nucleocapsid proteins inevitable genome encapsidation replication .  demonstrate interaction n protein hcov229e cyclophilin a cyclophilin b .  cyclophilin inhibitor .  .  . ',\n",
              "  'fda launches app health care professionals report novel uses existing medicines patients difficulttotreat infectious diseases'],\n",
              " ['regina yan s many traits open mind might one values all .  yan began nonprofit career managing international programs national academy sciences .  then surprise academys finance team invited work department .  what didnt know could learn .  said ok know think it doesnt hurt give try .  yan said .  i never looked back .  chief operating officer of category pharmaceuticals authors adele chapin pinworms red howler monkey  yue malauer yu zheng miroslav maleevi brigitte von brunn gunter fischer albrecht von brunnabstractthe wellknown immunosuppressive drug cyclosporin inhibits replication various viruses including coronaviruses binding cellular cyclophilins thus inactivating cistrans peptidylprolyl isomerase function .  viral nucleocapsid proteins inevitable genome encapsidation replication .  demonstrate interaction n protein hcov229e cyclophilin a cyclophilin b .  cyclophilin inhibitor .  .  . ',\n",
              "  'csuite awards regina yan'],\n",
              " ['cure id app allows clinicians share discuss experiences difficulttotreat infectious diseases website smartphone mobile device . fda approvals tags infectious diseases news alert pinworms red howler monkey  yue malauer yu zheng miroslav maleevi brigitte von brunn gunter fischer albrecht von brunnabstractthe wellknown immunosuppressive drug cyclosporin inhibits replication various viruses including coronaviruses binding cellular cyclophilins thus inactivating cistrans peptidylprolyl isomerase function .  viral nucleocapsid proteins inevitable genome encapsidation replication .  demonstrate interaction n protein hcov229e cyclophilin a cyclophilin b .  cyclophilin inhibitor .  .  . ',\n",
              "  'fda launches infectious disease crowdsourcing app clinicians fda launches infectious disease crowdsourcing app clinicians'],\n",
              " ['dsb composed representatives two fda centers eight federal government agencies agency healthcare research quality  yue malauer yu zheng miroslav maleevi brigitte von brunn gunter fischer albrecht von brunnabstractthe wellknown immunosuppressive drug cyclosporin inhibits replication various viruses including coronaviruses binding cellular cyclophilins thus inactivating cistrans peptidylprolyl isomerase function .  viral nucleocapsid proteins inevitable genome encapsidation replication .  demonstrate interaction n protein hcov229e cyclophilin a cyclophilin b .  cyclophilin inhibitor .  .  . ',\n",
              "  'drug safety oversight board']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp0XZ3L2uGoK"
      },
      "source": [
        "## Section 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqU-tBcyuJTs"
      },
      "source": [
        "MAX_LENGTH = max_len_text\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return input_tensor, target_tensor\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDwRIxw1ulTv"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout=0.2, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout = dropout\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG1OgEGnuciy"
      },
      "source": [
        "def trainIters(encoder, decoder, num_iters, learning_rate=0.05):\n",
        "    print(\"Starting training\")\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(num_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    losses = []\n",
        "    for i in range(1, num_iters + 1):\n",
        "        if i % 1000 == 0:\n",
        "            print(i, \"/\", num_iters + 1)\n",
        "        training_pair = training_pairs[i - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor,\n",
        "                     encoder, decoder,\n",
        "                     encoder_optimizer, decoder_optimizer, criterion)\n",
        "        losses.append(loss)\n",
        "\n",
        "    print(\"Ending training\")\n",
        "    return losses"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "kSXQwmbzuTMI",
        "outputId": "93e1915b-e3cb-4f1a-93a5-9c4df32fdf95"
      },
      "source": [
        "def train(input_tensor, target_tensor,\n",
        "          encoder, decoder,\n",
        "          encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "        encoder_outputs[i] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    teacher_forcing_ratio = 0.5\n",
        "    random_number = np.round(random.random(), 3)\n",
        "    if random_number < teacher_forcing_ratio:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,\n",
        "                                                                        decoder_hidden,\n",
        "                                                                        encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    # TODO: randomly generate a number between 0 and 1. If that number is greater than teacher_forcing_ratio, we will apply teacher forcing technique, otherwise not.\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "hidden_size = 300\n",
        "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoder(hidden_size, output_lang.n_words, dropout=0.1).to(device)\n",
        "\n",
        "trainIters(encoder, decoder, 1000)\n",
        "\n",
        "\n",
        "def infer(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with open(\"evaluation_input.txt\", 'w') as f:\n",
        "        for pair in pairs:\n",
        "            output_words, attentions = infer(encoder, decoder, pair[0])\n",
        "            output_sentence = ' '.join(output_words)\n",
        "            f.write(pair[1] + \",\" + output_sentence + \"\\n\")\n",
        "            # For every pair, write the target pair pair[1] and output_sentence as a tuple to\n",
        "            # a file.Let’s name this file as evaluation_input.txt"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-4e741dee597a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-891c5cf32022>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss = train(input_tensor, target_tensor,\n\u001b[1;32m     18\u001b[0m                      \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                      encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-4e741dee597a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m             decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,\n\u001b[1;32m     25\u001b[0m                                                                         \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                                                         encoder_outputs)\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
          ]
        }
      ]
    }
  ]
}